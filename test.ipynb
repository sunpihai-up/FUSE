{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from util.metric import eval_disparity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    print(sum(p.numel() for p in model.parameters()))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.depth_anything_v2.dpt_align_lora import DepthAnythingV2_lora\n",
    "student_model = DepthAnythingV2_lora(encoder=\"vitl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(student_model.pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = \"/data/coding/code/da2-prompt-tuning/vis_feature_maps/align_eventscape_event_1036_1_da2/npy/05_084_0124_image.npy\"\n",
    "d2 = \"/data/coding/code/da2-prompt-tuning/vis_feature_maps/align_eventscape_event_1036_1/npy/05_084_0124_image.npy\"\n",
    "\n",
    "pred = torch.tensor(np.load(d2))\n",
    "target = torch.tensor(np.load(d1))\n",
    "mask = torch.ones_like(pred, dtype=bool)\n",
    "pred = pred[mask]\n",
    "target = target[mask]\n",
    "pred = 1.0 / (pred + 1e-8)\n",
    "target = 1.0 / (target + 1e-8)\n",
    "# print(pred.shape, target.shape)\n",
    "# thresh = torch.max((target / pred), (pred / target))\n",
    "# # print(len(thresh))\n",
    "# d1 = torch.sum(thresh < 1.25).float()\n",
    "# print(d1)\n",
    "# # print(len(d1))\n",
    "print(eval_disparity(pred, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = r\"/home/sph/event/da2-prompt-tuning/exp/epde_nl_mvsec_2_decoder_20250116_222954/latest.pth\"\n",
    "state_dict = torch.load(p)\n",
    "# print(state_dict.keys())\n",
    "\n",
    "# previous_best = state_dict[\"previous_best\"]\n",
    "# print(previous_best)\n",
    "state_dict = state_dict['model']\n",
    "print(\"Model's state_dict keys:\")\n",
    "for key in state_dict.keys():\n",
    "    # print(state_dict[key].dtype)\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_para(module):\n",
    "    for name, param in module.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"Module: {name}, Trainable Parameters: {param.numel()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loralib as lora\n",
    "from torch import nn\n",
    "\n",
    "embed_dim = 1024\n",
    "linear = nn.Linear(embed_dim, embed_dim)\n",
    "linear_lora = lora.Linear(embed_dim, embed_dim, r=8)\n",
    "\n",
    "print_para(linear)\n",
    "print(\"*****************************\")\n",
    "print_para(linear_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_chans = 3\n",
    "patch_HW = (14,14)\n",
    "proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_HW, stride=patch_HW)\n",
    "proj_lora = lora.Conv2d(in_chans, embed_dim, kernel_size=patch_HW[0], stride=patch_HW[0], r=8)\n",
    "\n",
    "print_para(proj)\n",
    "print(\"*****************************\")\n",
    "print_para(proj_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(embed_dim, embed_dim*3)\n",
    "linear_lora = lora.Linear(embed_dim, embed_dim*3, r=8)\n",
    "print_para(linear)\n",
    "print(\"*****************************\")\n",
    "print_para(linear_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(embed_dim, embed_dim*3)\n",
    "\n",
    "# Alternatively, use lora.MergedLinear (recommended)\n",
    "qkv_proj = lora.MergedLinear(embed_dim, 3*embed_dim, r=8, enable_lora=[True, True, True], bias=True)\n",
    "\n",
    "print_para(linear)\n",
    "print(\"*****************************\")\n",
    "print_para(qkv_proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = lora.Linear(10, 10)\n",
    "m2 = lora.MergedLinear(10, 20, enable_lora=[True, True])\n",
    "m3 = lora.Conv2d(10, 10, 1)\n",
    "\n",
    "if isinstance(m1, nn.Linear):\n",
    "    print(\"m1\")\n",
    "if isinstance(m2, nn.Linear):\n",
    "    print(\"m2\")\n",
    "if isinstance(m3, nn.Conv2d):\n",
    "    print(\"m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "p = '/data/coding/code/da2-prompt-tuning/da2_metric_depth/exp/eventscape_voxel_nl_disp_da2vitl_20250107_000222/abs_rel-0.15593145787715912-0.pth'\n",
    "p = '/data/coding/code/da2-prompt-tuning/da2_metric_depth/exp/eventscape_voxel_nl_disp_da2vitl_20250107_000222/latest.pth'\n",
    "p = '/data/coding/upload-data/checkpoints/depth_anything_v2_vitl.pth'\n",
    "state_dict = torch.load(p, map_location=\"cpu\")\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = state_dict['model']\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def gen_split(scene, data_dir, output_dir=\"./dataset/splits/mvsec/\"):\n",
    "    depths_dir = os.path.join(data_dir, \"depths\")\n",
    "    images_dir = os.path.join(data_dir, \"images\")\n",
    "    voxels_dir = os.path.join(data_dir, \"voxels\")\n",
    "    \n",
    "    depths = os.listdir(depths_dir)\n",
    "    images = os.listdir(images_dir)\n",
    "    voxels = os.listdir(voxels_dir)\n",
    "    \n",
    "    depths.sort()\n",
    "    images.sort()\n",
    "    voxels.sort()\n",
    "    \n",
    "    assert len(depths) == len(images) and len(depths) == len(voxels)\n",
    "    \n",
    "    depths = [os.path.join(depths_dir, file_name) for file_name in depths]\n",
    "    images = [os.path.join(images_dir, file_name) for file_name in images]\n",
    "    voxels = [os.path.join(voxels_dir, file_name) for file_name in voxels]\n",
    "    \n",
    "    lines = []\n",
    "    for i in range(len(depths)):\n",
    "        line = images[i] + ' ' + depths[i] + ' ' + voxels[i] + '\\n'\n",
    "        lines.append(line)\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    if scene == \"outdoor_day2\": # Train DataSet\n",
    "        with open(output_dir + \"train.txt\", 'w') as f:\n",
    "            f.writelines(lines)\n",
    "    else:\n",
    "        # Test & Val DataSet\n",
    "        # TODO: \n",
    "        with open(output_dir + scene + \".txt\", 'a') as f:\n",
    "            f.writelines(lines)\n",
    "        \n",
    "        # lines = random.sample(lines, 100)\n",
    "        # with open(output_dir + scene + \"_val.txt\", 'a') as f:\n",
    "        #     f.writelines(lines)\n",
    "    print(f\"Saved split file to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_list = [\n",
    "        # (\"outdoor_day1_data.hdf5\", \"outdoor_day1_gt.hdf5\"),\n",
    "        # (\"outdoor_day2_data.hdf5\", \"outdoor_day2_gt.hdf5\"),\n",
    "        # (\"outdoor_night1_data.hdf5\", \"outdoor_night1_gt.hdf5\"),\n",
    "        (\"outdoor_night2_data.hdf5\", \"outdoor_night2_gt.hdf5\"),\n",
    "        (\"outdoor_night3_data.hdf5\", \"outdoor_night3_gt.hdf5\"),\n",
    "]\n",
    "\n",
    "root = '/data/coding/upload-data/data/mvsec'\n",
    "for pair in process_list:\n",
    "    # outdoor_day1, outdoor_day2 ...\n",
    "    scene = pair[0].split('_')[0] + '_' + pair[0].split('_')[1]\n",
    "\n",
    "    output_dir = os.path.join(root, scene)\n",
    "    gen_split(scene, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from model.epde.utils import token2feature, feature2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureFusionWeight(nn.Module):\n",
    "    def __init__(self, in_channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        weight_layers = [\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=in_channels,\n",
    "                kernel_size=kernel_size,\n",
    "            )\n",
    "        ]\n",
    "        weight_layers.append(nn.GELU())\n",
    "        weight_layers.append(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=2,\n",
    "                kernel_size=kernel_size,\n",
    "            )\n",
    "        )\n",
    "        self.get_weight = nn.Sequential(*weight_layers)\n",
    "    \n",
    "    def forward(self, joint_feas):\n",
    "        # The shape of x1 and x2 is [B, C, H, W]\n",
    "        weight = self.get_weight(joint_feas)\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "        return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, dropout=0.1):\n",
    "        super(CrossAttentionBlock, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        assert self.head_dim * num_heads == dim, \"dim must be divisible by num_heads\"\n",
    "        self.scale = self.head_dim**-0.5\n",
    "\n",
    "        self.query = nn.Linear(dim, dim)\n",
    "        self.key = nn.Linear(dim, dim)\n",
    "        self.value = nn.Linear(dim, dim)\n",
    "        self.out = nn.Linear(dim, dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        B, L, C = query.shape\n",
    "\n",
    "        # (B, num_heads, L, head_dim)\n",
    "        Q = self.query(query).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.query(key).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value(value).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention (B, num_heads, L, L)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        attn_weight = F.softmax(scores, dim=-1)\n",
    "        attn_weight = self.dropout(attn_weight)\n",
    "\n",
    "        # (B, num_heads, L, head_dim)\n",
    "        context = torch.matmul(attn_weight, V)\n",
    "        # (B, L, dim)\n",
    "        context = context.transpose(1, 2).contiguous().view(B, L, self.dim)\n",
    "\n",
    "        return self.out(context)\n",
    "\n",
    "\n",
    "class FeatureInteraction(nn.Module):\n",
    "    def __init__(self, dim, num_heads, dropout=0.1):\n",
    "        super(FeatureInteraction, self).__init__()\n",
    "        self.cross_attention_1 = CrossAttentionBlock(dim, num_heads, dropout)\n",
    "        self.cross_attention_2 = CrossAttentionBlock(dim, num_heads, dropout)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        inter_x1 = self.cross_attention_1(query=x1, key=x2, value=x2)\n",
    "        inter_x2 = self.cross_attention_2(query=x2, key=x1, value=x1)\n",
    "        \n",
    "        return inter_x1, inter_x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialWeights(nn.Module):\n",
    "    def __init__(self, dim, reduction=1):\n",
    "        super(SpatialWeights, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv2d(self.dim * 2, self.dim // reduction, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(self.dim // reduction, 2, kernel_size=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B, _, H, W = x1.shape\n",
    "        # x = torch.cat((x1, x2), dim=1)  # B 2C H W\n",
    "        return self.mlp(x)\n",
    "    # def forward(self, x1, x2):\n",
    "    #     B, _, H, W = x1.shape\n",
    "    #     x = torch.cat((x1, x2), dim=1)  # B 2C H W\n",
    "    #     return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureFusionModule(nn.Module):\n",
    "    def __init__(self, dim, num_heads=None, norm_layer=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.PixelShuffle(upscale_factor=2)\n",
    "        self.interact = FeatureInteraction(dim=dim//4, num_heads=num_heads)\n",
    "        self.downsample_1 = nn.Conv2d(\n",
    "            in_channels=dim // 4, out_channels=dim, kernel_size=1, stride=2\n",
    "        )\n",
    "        self.downsample_2 = nn.Conv2d(\n",
    "            in_channels=dim // 4, out_channels=dim, kernel_size=1, stride=2\n",
    "        )\n",
    "        self.fuse_weight = FeatureFusionWeight(\n",
    "            in_channels=dim // 2, kernel_size=1\n",
    "        )\n",
    "    \n",
    "    def fuse_feature_maps(self, weight_map, feature_map1, feature_map2):\n",
    "        # Split the weight map into two components, [B, 1, H, W]\n",
    "        weight1 = weight_map[:, 0:1, :, :]\n",
    "        weight2 = weight_map[:, 1:2, :, :]\n",
    "        \n",
    "        # Apply weights to the feature maps, [B, C, H, W] * [B, 1, H, W]\n",
    "        weighted_feature1 = feature_map1 * weight1\n",
    "        weighted_feature2 = feature_map2 * weight2\n",
    "\n",
    "        # Compute the fused feature map, [B, C, H, W]        \n",
    "        return weighted_feature1 + weighted_feature2\n",
    "\n",
    "    def forward(self, x1, x2, patch_grid_size):\n",
    "        assert x1.shape == x2.shape, \"The shape of x1 and x2 does not match.\"\n",
    "        # [B, L, C] --> [B, C, H, W]\n",
    "        x1 = token2feature(x1, patch_grid_size)\n",
    "        x2 = token2feature(x2, patch_grid_size)\n",
    "\n",
    "        # [B, C, H, W] --> [B, C/4, H*2, W*2]\n",
    "        H, W = patch_grid_size\n",
    "        upsample_patch_grid_size = (H * 2, W * 2)\n",
    "        x1 = self.upsample(x1)\n",
    "        x2 = self.upsample(x2)\n",
    "        \n",
    "        # [B, C/4, H*2, W*2] --> [B, L*4, C/4]\n",
    "        x1 = feature2token(x1)\n",
    "        x2 = feature2token(x2)\n",
    "        x1, x2 = self.interact(x1, x2)\n",
    "        \n",
    "        # [B, L*4, C/4] --> [B, C/4, H*2, W*2]\n",
    "        x1 = token2feature(x1, upsample_patch_grid_size)\n",
    "        x2 = token2feature(x2, upsample_patch_grid_size)\n",
    "        \n",
    "        # [B, C/4, H*2, W*2] --> [B, C, H, W]\n",
    "        x1_down = self.downsample_1(x1)\n",
    "        x2_down = self.downsample_2(x2)\n",
    "        \n",
    "        # [B, C/4, H*2, W*2] * 2 --> [B, C/2, H*2, W*2]\n",
    "        joint_feas = torch.cat((x1, x2), dim=1)\n",
    "        weight = self.fuse_weight(joint_feas)\n",
    "        fused_feature_map = self.fuse_feature_maps(weight, x1_down, x2_down)\n",
    "        return feature2token(fused_feature_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.epde.prompt_module import FeatureInteraction\n",
    "from model.epde.prompt_module import FeatureFusionWeight\n",
    "class FeatureFusionModule(nn.Module):\n",
    "    def __init__(self, dim, num_heads=None, reduction=1, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.channel_down_proj1 = nn.Linear(dim, dim // reduction)\n",
    "        self.channel_down_proj2 = nn.Linear(dim, dim // reduction)\n",
    "        self.interact = FeatureInteraction(dim=dim // reduction, num_heads=num_heads)\n",
    "        self.channel_up_proj1 = nn.Linear(dim // reduction, dim)\n",
    "        self.channel_up_proj2 = nn.Linear(dim // reduction, dim)\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.fuse_weight = SpatialWeights(dim=dim, reduction=reduction)\n",
    "    \n",
    "    def fuse_feature_maps(self, weight_map, feature_map1, feature_map2):\n",
    "        # Split the weight map into two components, [B, 1, H, W]\n",
    "        weight1 = weight_map[:, 0:1, :, :]\n",
    "        weight2 = weight_map[:, 1:2, :, :]\n",
    "        \n",
    "        # Apply weights to the feature maps, [B, C, H, W] * [B, 1, H, W]\n",
    "        weighted_feature1 = feature_map1 * weight1\n",
    "        weighted_feature2 = feature_map2 * weight2\n",
    "\n",
    "        # Compute the fused feature map, [B, C, H, W]        \n",
    "        return weighted_feature1 + weighted_feature2\n",
    "\n",
    "    def forward(self, x1, x2, patch_grid_size):\n",
    "        assert x1.shape == x2.shape, \"The shape of x1 and x2 does not match.\"\n",
    "        \n",
    "        y1 = self.channel_down_proj1(x1)\n",
    "        y2 = self.channel_down_proj2(x2)\n",
    "        \n",
    "        y1, y2 = self.interact(y1, y2)\n",
    "        \n",
    "        y1 = self.channel_up_proj1(y1)\n",
    "        y2 = self.channel_up_proj2(y2)\n",
    "        \n",
    "        print(y1.shape, x1.shape)\n",
    "        y1 = self.norm1(y1 + x1)\n",
    "        y2 = self.norm2(y2 + x2)\n",
    "        \n",
    "        y1 = token2feature(y1, patch_grid_size)\n",
    "        y2 = token2feature(y2, patch_grid_size)\n",
    "        \n",
    "        joint_feas = torch.cat((y1, y2), dim=1)\n",
    "        weight = self.fuse_weight(joint_feas)\n",
    "        fused_feature_map = self.fuse_feature_maps(weight, y1, y2)\n",
    "        return feature2token(fused_feature_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffm = FeatureFusionModule(dim=1024, num_heads=16, reduction=8)\n",
    "count_parameters(ffm.interact)\n",
    "count_parameters(ffm.fuse_weight)\n",
    "count_parameters(ffm.channel_down_proj1)\n",
    "count_parameters(ffm.channel_up_proj1)\n",
    "count_parameters(ffm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 4  # Batch size\n",
    "C = 1024  # Channels (e.g., RGB)\n",
    "H = 20 # Height\n",
    "W = 30 # Width\n",
    "embed_dim = C\n",
    "L = H * W\n",
    "# rgb = torch.randn(B, C, H, W)\n",
    "# eve = torch.randn(B, C, H, W)\n",
    "x1 = torch.randn(B, L, C)\n",
    "x2 = torch.randn(B, L, C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = ffm(x1, x2, (H, W))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = (2, 3)\n",
    "p = p * 2\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossPath(nn.Module):\n",
    "    def __init__(self, dim, reduction=1, num_heads=None, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.cross_attn = CrossAttention(dim // reduction, num_heads=num_heads)\n",
    "        # self.end_proj1 = nn.Linear(dim // reduction, dim)\n",
    "        # self.end_proj2 = nn.Linear(dim // reduction, dim)\n",
    "        # self.end_proj3 = nn.Linear(dim // reduction * 2, dim)\n",
    "        # self.norm1 = norm_layer(dim)\n",
    "        # self.norm2 = norm_layer(dim)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        v1, v2 = self.cross_attn(x1, x2)\n",
    "        # out_x1 = self.norm1(x1 + v1)\n",
    "        # out_x2 = self.norm2(x2 + v2)\n",
    "        # out = self.end_proj3(torch.cat((out_x1, out_x2), dim=-1))\n",
    "        return v1, v2\n",
    "        return out\n",
    "\n",
    "cro = CrossPath(dim=1024, num_heads=16, reduction=1)\n",
    "count_parameters(cro)\n",
    "\n",
    "m = nn.Conv2d(\n",
    "    in_channels=2048,\n",
    "    out_channels=1024,\n",
    "    kernel_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def token2feature(tokens, patch_grid_size):\n",
    "    \"\"\"add token transfer to feature\"\"\"\n",
    "    B, L, D = tokens.shape\n",
    "    # H = W = int(L**0.5)\n",
    "    H, W = patch_grid_size[0], patch_grid_size[1]\n",
    "    x = tokens.permute(0, 2, 1).view(B, D, H, W).contiguous()\n",
    "    return x\n",
    "\n",
    "\n",
    "def feature2token(x):\n",
    "    B, C, H, W = x.shape\n",
    "    L = W * H\n",
    "    tokens = x.view(B, C, L).permute(0, 2, 1).contiguous()\n",
    "    return tokens\n",
    "\n",
    "ffm = FeatureFusionModule(dim=1024, num_heads=16, reduction=1)\n",
    "# count_parameters(ffm)\n",
    "# count_parameters(ffm.cross)\n",
    "count_parameters(ffm.channel_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 4  # Batch size\n",
    "C = 1024  # Channels (e.g., RGB)\n",
    "H = 50 # Height\n",
    "W = 50 # Width\n",
    "N = 100\n",
    "embed_dim = C\n",
    "rgb = torch.randn(B, C, H, W)\n",
    "eve = torch.randn(B, C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.epde.prompt_module import FeatureFusionModule\n",
    "from model.epde.prompt_module import FeatureRectifyModule\n",
    "embed_dim = 1024\n",
    "upscale_factor = 2\n",
    "\n",
    "dim = embed_dim // (upscale_factor * upscale_factor)\n",
    "ffm = FeatureFusionModule(dim=dim, reduction=1, num_heads=16)\n",
    "frf = FeatureRectifyModule(dim=dim, reduction=1)\n",
    "pixel_shuffle = nn.PixelShuffle(upscale_factor=upscale_factor)\n",
    "\n",
    "print(rgb.shape)\n",
    "\n",
    "rgb = pixel_shuffle(rgb)\n",
    "eve = pixel_shuffle(eve)\n",
    "print(rgb.shape)\n",
    "\n",
    "# merge = ffm(rgb, eve)\n",
    "# print(merge.shape)\n",
    "count_parameters(ffm)\n",
    "count_parameters(frf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = ChannelEmbed(C * 2, C, reduction=1)\n",
    "count_parameters(merge.residual)\n",
    "count_parameters(merge)\n",
    "\n",
    "print(rgb.shape)\n",
    "cat_fea = torch.cat((rgb, eve), dim=-1)\n",
    "print(cat_fea.shape)\n",
    "fea = merge(cat_fea, H=10, W=10)\n",
    "print(fea.shape)\n",
    "token = feature2token(fea)\n",
    "print(token.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 2048\n",
    "out_channels = 1024\n",
    "reduction = 1\n",
    "m = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "count_parameters(m)\n",
    "m = nn.Conv2d(in_channels, out_channels // reduction, kernel_size=1, bias=True)\n",
    "count_parameters(m)\n",
    "m = nn.Conv2d(\n",
    "    out_channels // reduction,\n",
    "    out_channels // reduction,\n",
    "    kernel_size=3,\n",
    "    stride=1,\n",
    "    padding=1,\n",
    "    bias=True,\n",
    "    groups=out_channels // reduction,\n",
    ")\n",
    "count_parameters(m)\n",
    "m = nn.Conv2d(out_channels // reduction, out_channels, kernel_size=1, bias=True)\n",
    "count_parameters(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.depth_anything_v2.dinov2_layers.patch_embed import PatchEmbed\n",
    "img_size = (H, W)\n",
    "patch_size = (5, 5)\n",
    "patch_grid_size = (H // patch_size[0], W // patch_size[1])\n",
    "\n",
    "img = torch.randn(B, 3, H, W)\n",
    "patch_embed = PatchEmbed(\n",
    "    img_size=img_size,\n",
    "    patch_size=patch_size,\n",
    "    in_chans=3,\n",
    "    embed_dim=embed_dim,\n",
    ")\n",
    "tokens = patch_embed(img)\n",
    "print(tokens.shape)\n",
    "fea = token2feature(tokens=tokens, patch_grid_size=patch_grid_size)\n",
    "print(fea.shape)\n",
    "\n",
    "print(rgb.shape)\n",
    "rgb = rgb.flatten(2).transpose(1, 2)\n",
    "print(rgb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MM-SAM Selective Fusion Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "import math\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "def feature_fusion(\n",
    "    joint_feats: torch.Tensor, joint_feat_masks: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    # (B, N_patch, N_filter, feat_num)\n",
    "    joint_feat_masks = F.softmax(joint_feat_masks, dim=-1)\n",
    "    # (B, N_patch, N_filter, feat_num) -> (N_filter, B, N_patch, feat_num) -> (N_filter, B, N_patch, 1, feat_num)\n",
    "    joint_feat_masks = joint_feat_masks.permute(2, 0, 1, 3).unsqueeze(-2)\n",
    "\n",
    "    # (B, N_patch, C, feat_num) -> (1, B, N_patch, C, feat_num)\n",
    "    joint_feats = joint_feats.unsqueeze(0)\n",
    "\n",
    "    # (N_filter, B, N_patch, 1, feat_num) * (1, B, N_patch, C, feat_num) -> (N_filter, B, N_patch, C, feat_num)\n",
    "    joint_feats = joint_feats * joint_feat_masks\n",
    "    # weighted sum: (N_filter, B, N_patch, C, feat_num) -> (N_filter, B, N_patch, C)\n",
    "    joint_feats = torch.sum(joint_feats, dim=-1)\n",
    "    # filter average: (N_filter, B, N_patch, C) -> (B, N_patch, C)\n",
    "    joint_feats = torch.mean(joint_feats, dim=0)\n",
    "    return joint_feats\n",
    "\n",
    "\n",
    "def feature_fusion_by_rgb_x(\n",
    "    rgb_feats: torch.Tensor,\n",
    "    x_feats: torch.Tensor,\n",
    "    rgb_feat_masks: torch.Tensor,\n",
    "    x_feat_masks: torch.Tensor,\n",
    "):\n",
    "    # (B, N_patch, N_filter, 1) * 2 -> (B, N_patch, N_filter, 2)\n",
    "    joint_feat_masks = torch.cat(\n",
    "        (rgb_feat_masks.unsqueeze(-1), x_feat_masks.unsqueeze(-1)), dim=-1\n",
    "    )\n",
    "    # (B, N_patch, C, 1) * 2 -> (B, N_patch, C, 2)\n",
    "    joint_feats = torch.cat((rgb_feats.unsqueeze(-1), x_feats.unsqueeze(-1)), dim=-1)\n",
    "    return feature_fusion(joint_feats, joint_feat_masks)\n",
    "\n",
    "\n",
    "class SelectiveFusionGate(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels_per_feat: int = 256,\n",
    "        filter_num: int = 1,\n",
    "        feat_num: int = 2,\n",
    "        intermediate_channels: Union[int, List[int], None] = None,\n",
    "        filter_type: str = \"linear\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if intermediate_channels is None:\n",
    "            intermediate_channels = []\n",
    "        if isinstance(intermediate_channels, int):\n",
    "            intermediate_channels = [intermediate_channels]\n",
    "\n",
    "        in_channels = in_channels_per_feat * feat_num\n",
    "        if filter_type == \"linear\":\n",
    "            filter_bank, layer_in_features = [], in_channels\n",
    "            for layer_out_features in intermediate_channels:\n",
    "                filter_bank.extend(\n",
    "                    [\n",
    "                        nn.Linear(\n",
    "                            in_features=layer_in_features,\n",
    "                            out_features=layer_out_features,\n",
    "                        ),\n",
    "                        nn.GELU(),\n",
    "                    ]\n",
    "                )\n",
    "                layer_in_features = layer_out_features\n",
    "            filter_bank.append(\n",
    "                nn.Linear(\n",
    "                    in_features=layer_in_features, out_features=filter_num * feat_num\n",
    "                )\n",
    "            )\n",
    "            self.filter_bank = nn.Sequential(*filter_bank)\n",
    "\n",
    "        elif filter_type == \"conv2d\":\n",
    "            self.filter_bank = Conv2dFilterBank(\n",
    "                in_channels=in_channels,\n",
    "                filter_num=filter_num * feat_num,\n",
    "                intermediate_channels=intermediate_channels,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Filter type {filter_type} not implemented!\")\n",
    "\n",
    "    def forward(self, feat_list: List[torch.Tensor]) -> (torch.Tensor, torch.Tensor):\n",
    "        # (B, C, H, W, feat_num)\n",
    "        joint_feats = torch.stack(feat_list, dim=-1)\n",
    "\n",
    "        # (B, C, H, W, feat_num) -> (B, C, N_patch, feat_num) -> (B, N_patch, feat_num, C)\n",
    "        # Note: feat_num should be in front of C for correct concatenation\n",
    "        bs, c, h, w, feat_num = joint_feats.shape\n",
    "        joint_feats = joint_feats.flatten(2, 3).permute(0, 2, 3, 1)\n",
    "        # (B, N_patch, feat_num, C) -> (B, N_patch, C * feat_num)\n",
    "        joint_feats = joint_feats.reshape(bs, h * w, -1)\n",
    "\n",
    "        # filter mask generation: (B, N_patch, C * feat_num) -> (B, N_patch, N_filter * feat_num)\n",
    "        joint_feat_masks = self.filter_bank(joint_feats)\n",
    "        # (B, N_patch, N_filter * feat_num) -> (B, N_patch, N_filter, feat_num)\n",
    "        joint_feat_masks = joint_feat_masks.reshape(bs, h * w, -1, feat_num)\n",
    "\n",
    "        # (B, N_patch, C * feat_num) -> (B, N_patch, feat_num, C) -> (B, N_patch, C, feat_num)\n",
    "        joint_feats = joint_feats.reshape(bs, h * w, feat_num, -1).permute(0, 1, 3, 2)\n",
    "\n",
    "        # (B, N_patch, C)\n",
    "        joint_feats = feature_fusion(joint_feats, joint_feat_masks)\n",
    "\n",
    "        # back to patch style: (B, N_patch, C) -> (B, C, H, W)\n",
    "        joint_feats = joint_feats.permute(0, 2, 1).reshape(bs, c, h, w)\n",
    "        # (B, N_patch, N_filter, feat_num) -> (B, H, W, N_filter, feat_num)\n",
    "        filter_num = joint_feat_masks.size(-2)\n",
    "        joint_feat_masks = joint_feat_masks.reshape(bs, h, w, filter_num, feat_num)\n",
    "        return joint_feats, joint_feat_masks\n",
    "\n",
    "\n",
    "class Conv2dFilterBank(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 256,\n",
    "        filter_num: int = 1,\n",
    "        kernel_size: int = 3,\n",
    "        intermediate_channels: Union[int, List[int], None] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert kernel_size % 2 == 1, \"kernel_size should be odd number!\"\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        conv_filter_bank, layer_in_channels = [], in_channels\n",
    "        print(layer_in_channels)\n",
    "        for layer_out_channels in intermediate_channels:\n",
    "            conv_filter_bank.extend(\n",
    "                [\n",
    "                    nn.Conv2d(\n",
    "                        in_channels=layer_in_channels,\n",
    "                        out_channels=layer_out_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=padding,\n",
    "                    ),\n",
    "                    nn.GELU(),\n",
    "                ]\n",
    "            )\n",
    "            layer_in_channels = layer_out_channels\n",
    "        conv_filter_bank.append(\n",
    "            nn.Conv2d(\n",
    "                in_channels=layer_in_channels,\n",
    "                out_channels=filter_num,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "            )\n",
    "        )\n",
    "        self.conv_filter_bank = nn.Sequential(*conv_filter_bank)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # get the height and width for reshaping\n",
    "        side_len = int(math.sqrt(x.shape[-2]))\n",
    "        # (B, N_patch, C) -> (B, C, N_patch) -> (B, C, H, W)\n",
    "        x = x.permute(0, 2, 1).reshape(x.shape[0], -1, side_len, side_len)\n",
    "        # (B, C, H, W) -> (B, N_filter, H, W)\n",
    "        x = self.conv_filter_bank(x)\n",
    "        # (B, N_filter, H, W) -> (B, N_filter, N_patch) -> (B, N_patch, N_filter)\n",
    "        x = x.flatten(2).permute(0, 2, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_module = SelectiveFusionGate(\n",
    "    in_channels_per_feat=1024,\n",
    "    filter_num=1,\n",
    "    intermediate_channels=1024,\n",
    "    filter_type=\"conv2d\",\n",
    ")\n",
    "count_parameters(fusion_module)\n",
    "m = nn.Conv2d(\n",
    "    in_channels=512,\n",
    "    out_channels=256,\n",
    "    kernel_size=1,\n",
    "    padding=(1//2),\n",
    ")\n",
    "count_parameters(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Feature Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.epde.prompt_module import Prompt_block, Fovea\n",
    "from model.epde.utils import token2feature, feature2token\n",
    "from util.vis import vis_feature_var, vis_depth_map\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vis_depth_map(dep, show_colorbar=False, cmap_name=\"Spectral\", save_fig=False, save_path=\"test.png\"):\n",
    "#     cmap = matplotlib.colormaps.get_cmap(cmap_name)\n",
    "\n",
    "#     fig, ax = plt.subplots()\n",
    "#     fig.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     im = ax.imshow(dep, cmap=cmap, extent=[0, dep.shape[1], 0, dep.shape[0]])\n",
    "#     if show_colorbar:\n",
    "#         plt.colorbar(im)\n",
    "\n",
    "#     dpi = fig.get_dpi()\n",
    "#     fig.set_size_inches(dep.shape[1] / dpi, dep.shape[0] / dpi)\n",
    "\n",
    "#     if save_fig:\n",
    "#         plt.savefig(save_path)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EventScape\n",
    "# patch_size = (38, 19) # 266\n",
    "# patch_size = (50, 25) # 350\n",
    "# patch_size = (74, 37) # 518 \n",
    "# patch_size = (148, 74) # 1036\n",
    "\n",
    "# MVSEC\n",
    "# patch_size = (25, 19) # 266\n",
    "# patch_size = (49, 37) # 518\n",
    "patch_size = (98, 74) # 1036\n",
    "\n",
    "# DSEC\n",
    "# patch_size = (99, 74)# 1036\n",
    "patch_size = (patch_size[1], patch_size[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fovea = Fovea()\n",
    "pixel_shuffle = nn.PixelShuffle(upscale_factor=2)\n",
    "pixel_unshuffle = nn.PixelUnshuffle(downscale_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/data/coding/code/da2-prompt-tuning/vis_feature_maps/align_log_l1andfea_eventscape_mxsec_1036_2_lora/fea/04056_0.png.npy\"\n",
    "tokens = torch.from_numpy(np.load(path)).unsqueeze(0)\n",
    "feas = token2feature(tokens=tokens, patch_grid_size=patch_size)\n",
    "var = torch.var(feas, dim=1, keepdim=True).squeeze().numpy()\n",
    "vis_depth_map(var, cmap_name=\"viridis\")\n",
    "\n",
    "# feas = fovea(feas)\n",
    "# var = torch.var(feas, dim=1, keepdim=True).squeeze().numpy()\n",
    "# vis_depth_map(var, cmap_name=\"viridis\")\n",
    "\n",
    "print(feas.shape)\n",
    "feas = pixel_shuffle(feas)\n",
    "print(feas.shape)\n",
    "var = torch.var(feas, dim=1, keepdim=True).squeeze().numpy()\n",
    "vis_depth_map(var, cmap_name=\"viridis\")\n",
    "\n",
    "print(feas.shape)\n",
    "feas = pixel_unshuffle(feas)\n",
    "print(feas.shape)\n",
    "var = torch.var(feas, dim=1, keepdim=True).squeeze().numpy()\n",
    "vis_depth_map(var, cmap_name=\"viridis\")\n",
    "# tokens = feature2token(feas)\n",
    "# print(tokens.shape)\n",
    "# vis_feature_var(tokens, patch_size=patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_keywords = [\n",
    "    f\"_abs_rel_diff\",\n",
    "    f\"_squ_rel_diff\",\n",
    "    f\"_RMS_linear\",\n",
    "    f\"_RMS_log\",\n",
    "    f\"_SILog\",\n",
    "    f\"_mean_target_depth\",\n",
    "    f\"_median_target_depth\",\n",
    "    f\"_mean_prediction_depth\",\n",
    "    f\"_median_prediction_depth\",\n",
    "    f\"_mean_depth_error\",\n",
    "    f\"_median_diff\",\n",
    "    f\"_threshold_delta_1.25\",\n",
    "    f\"_threshold_delta_1.25^2\",\n",
    "    f\"_threshold_delta_1.25^3\",\n",
    "    f\"_10_mean_target_depth\",\n",
    "    f\"_10_median_target_depth\",\n",
    "    f\"_10_mean_prediction_depth\",\n",
    "    f\"_10_median_prediction_depth\",\n",
    "    f\"_10_abs_rel_diff\",\n",
    "    f\"_10_squ_rel_diff\",\n",
    "    f\"_10_RMS_linear\",\n",
    "    f\"_10_RMS_log\",\n",
    "    f\"_10_SILog\",\n",
    "    f\"_10_mean_depth_error\",\n",
    "    f\"_10_median_diff\",\n",
    "    f\"_10_threshold_delta_1.25\",\n",
    "    f\"_10_threshold_delta_1.25^2\",\n",
    "    f\"_10_threshold_delta_1.25^3\",\n",
    "    f\"_20_abs_rel_diff\",\n",
    "    f\"_20_squ_rel_diff\",\n",
    "    f\"_20_RMS_linear\",\n",
    "    f\"_20_RMS_log\",\n",
    "    f\"_20_SILog\",\n",
    "    f\"_20_mean_target_depth\",\n",
    "    f\"_20_median_target_depth\",\n",
    "    f\"_20_mean_prediction_depth\",\n",
    "    f\"_20_median_prediction_depth\",\n",
    "    f\"_20_mean_depth_error\",\n",
    "    f\"_20_median_diff\",\n",
    "    f\"_20_threshold_delta_1.25\",\n",
    "    f\"_20_threshold_delta_1.25^2\",\n",
    "    f\"_20_threshold_delta_1.25^3\",\n",
    "    f\"_30_abs_rel_diff\",\n",
    "    f\"_30_squ_rel_diff\",\n",
    "    f\"_30_RMS_linear\",\n",
    "    f\"_30_RMS_log\",\n",
    "    f\"_30_SILog\",\n",
    "    f\"_30_mean_target_depth\",\n",
    "    f\"_30_median_target_depth\",\n",
    "    f\"_30_mean_prediction_depth\",\n",
    "    f\"_30_median_prediction_depth\",\n",
    "    f\"_30_mean_depth_error\",\n",
    "    f\"_30_median_diff\",\n",
    "    f\"_30_threshold_delta_1.25\",\n",
    "    f\"_30_threshold_delta_1.25^2\",\n",
    "    f\"_30_threshold_delta_1.25^3\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_metrics(\n",
    "    idx,\n",
    "    metrics,\n",
    "    target_,\n",
    "    prediction_,\n",
    "    mask,\n",
    "    event_frame=None,\n",
    "    prefix=\"\",\n",
    "    debug=False,\n",
    "    output_folder=None,\n",
    "):\n",
    "    if len(metrics) == 0:\n",
    "        metrics = {k: 0 for k in metrics_keywords}\n",
    "\n",
    "    prediction_mask = (prediction_ > 0) & (\n",
    "        prediction_ <= np.amax(target_[~np.isnan(target_)])\n",
    "    )\n",
    "    depth_mask = (target_ > 0) & (\n",
    "        target_ <= np.amax(target_[~np.isnan(target_)])\n",
    "    )  # make (target> 3) for mvsec might drives\n",
    "    mask = mask & depth_mask & prediction_mask\n",
    "    eps = 1e-5\n",
    "\n",
    "    target = target_[\n",
    "        mask\n",
    "    ]  # np.where(mask, target_, np.max(target_[~np.isnan(target_)]))# target_[mask] but without lossing shape\n",
    "    prediction = prediction_[\n",
    "        mask\n",
    "    ]  # np.where(mask, prediction_, np.max(target_[~np.isnan(target_)]))# prediction_[mask] but without lossing shape\n",
    "\n",
    "    # thresholds\n",
    "    # ratio = np.max(np.stack([target/(prediction+eps),prediction/(target+eps)]), axis=0)\n",
    "    ratio = np.maximum((target / (prediction + eps)), (prediction / (target + eps)))\n",
    "\n",
    "    new_metrics = {}\n",
    "    new_metrics[f\"{prefix}threshold_delta_1.25\"] = np.mean(ratio <= 1.25)\n",
    "    new_metrics[f\"{prefix}threshold_delta_1.25^2\"] = np.mean(ratio <= 1.25**2)\n",
    "    new_metrics[f\"{prefix}threshold_delta_1.25^3\"] = np.mean(ratio <= 1.25**3)\n",
    "\n",
    "    # abs diff\n",
    "    # log_diff = np.log(target+eps)-np.log(prediction+eps)\n",
    "    log_diff = np.log(prediction + eps) - np.log(target + eps)\n",
    "    # log_diff = np.abs(log_target - log_prediction)\n",
    "    abs_diff = np.abs(target - prediction)\n",
    "\n",
    "    new_metrics[f\"{prefix}abs_rel_diff\"] = (abs_diff / (target + eps)).mean()\n",
    "    # new_metrics[f\"{prefix}squ_rel_diff\"] = (abs_diff**2 / (target**2 + eps)).mean()\n",
    "    new_metrics[f\"{prefix}squ_rel_diff\"] = (abs_diff**2 / (target + eps)).mean()\n",
    "    new_metrics[f\"{prefix}RMS_linear\"] = np.sqrt((abs_diff**2).mean())\n",
    "    new_metrics[f\"{prefix}RMS_log\"] = np.sqrt((log_diff**2).mean())\n",
    "    new_metrics[f\"{prefix}SILog\"] = (log_diff**2).mean() - (log_diff.mean()) ** 2\n",
    "    new_metrics[f\"{prefix}mean_target_depth\"] = target.mean()\n",
    "    new_metrics[f\"{prefix}median_target_depth\"] = np.median(target)\n",
    "    new_metrics[f\"{prefix}mean_prediction_depth\"] = prediction.mean()\n",
    "    new_metrics[f\"{prefix}median_prediction_depth\"] = np.median(prediction)\n",
    "    new_metrics[f\"{prefix}mean_depth_error\"] = abs_diff.mean()\n",
    "    new_metrics[f\"{prefix}median_diff\"] = np.abs(\n",
    "        np.median(target) - np.median(prediction)\n",
    "    )\n",
    "\n",
    "    for k, v in new_metrics.items():\n",
    "        metrics[k] += v\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2params = {\n",
    "    \"dense\": {\"clip_distance\": 1000.0, \"reg_factor\": 6.2044},\n",
    "    \"mvsec\": {\"clip_distance\": 80.0, \"reg_factor\": 3.70378},\n",
    "    \"eventscape\": {\"clip_distance\": 1000.0, \"reg_factor\": 5.7},\n",
    "}\n",
    "reg_factor = dataset2params[\"mvsec\"][\"reg_factor\"]\n",
    "max_depth = dataset2params[\"mvsec\"][\"clip_distance\"]\n",
    "crop_ymax = 260\n",
    "nan_mask = True\n",
    "\n",
    "p_file = \"/home/sph/event/da2-prompt-tuning/results/test/epde_metric_80_mvsec_2_decoder_116131204_mvsec_night1_7/npy/00020.npy\"\n",
    "t_file = \"/data_nvme/sph/mvsec_processed/outdoor_night1/depths/00020.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_depth = np.exp(-1 * reg_factor) * max_depth\n",
    "\n",
    "# Read absolute scale ground truth\n",
    "target_depth = np.load(t_file)\n",
    "# Crop depth height according to argument\n",
    "target_depth = target_depth[: crop_ymax]\n",
    "\n",
    "# Read predicted depth data\n",
    "predicted_depth = np.load(p_file)\n",
    "# Crop depth height according to argument\n",
    "predicted_depth = predicted_depth[: crop_ymax]\n",
    "\n",
    "if nan_mask:\n",
    "    non_nan_mask = ~np.isnan(target_depth)\n",
    "    target_depth = target_depth[non_nan_mask]\n",
    "    predicted_depth = predicted_depth[non_nan_mask]\n",
    "\n",
    "assert target_depth.ndim <= 2\n",
    "assert predicted_depth.ndim <= 2\n",
    "\n",
    "# Flat numpy arrary\n",
    "if target_depth.ndim == 2:\n",
    "    target_depth = target_depth.ravel()\n",
    "    predicted_depth = predicted_depth.ravel()\n",
    "\n",
    "assert predicted_depth.shape == target_depth.shape\n",
    "\n",
    "depth_mask = np.ones_like(target_depth) > 0\n",
    "# Clip\n",
    "target_depth = np.clip(target_depth, min_depth, max_depth)\n",
    "predicted_depth = np.clip(predicted_depth, min_depth, max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.from_numpy(np.load(t_file))\n",
    "pred = torch.from_numpy(np.load(p_file))\n",
    "\n",
    "# non_nan_mask = torch.isfinite(target)\n",
    "# target = target[non_nan_mask]\n",
    "# pred = pred[non_nan_mask]\n",
    "\n",
    "# min_depth = torch.exp(-1 * torch.tensor(reg_factor)) * torch.tensor(max_depth)\n",
    "\n",
    "# target = torch.clamp(target, min=min_depth, max=max_depth)\n",
    "# pred = torch.clamp(pred, min=min_depth, max=max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_depth = torch.from_numpy(target_depth)\n",
    "# predicted_depth = torch.from_numpy(predicted_depth)\n",
    "\n",
    "tar_diff = target_depth - target\n",
    "pre_diff = predicted_depth - pred\n",
    "\n",
    "print(tar_diff.abs().max(), pre_diff.abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "metrics = add_to_metrics(\n",
    "            0,\n",
    "            metrics,\n",
    "            target_depth,\n",
    "            predicted_depth,\n",
    "            depth_mask,\n",
    "            event_frame=None,\n",
    "            prefix=\"_\",\n",
    "            debug=False,\n",
    "            output_folder=None,\n",
    "        )\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred.min(), target.min(), pred.max(), target.max())\n",
    "\n",
    "non_nan_mask = torch.isfinite(target)\n",
    "target = target[non_nan_mask]\n",
    "pred = pred[non_nan_mask]\n",
    "print(pred.min(), target.min(), pred.max(), target.max())\n",
    "\n",
    "reg_factor = dataset2params[\"mvsec\"][\"reg_factor\"]\n",
    "max_depth = dataset2params[\"mvsec\"][\"clip_distance\"]\n",
    "min_depth = torch.exp(-1 * torch.tensor(reg_factor)) * torch.tensor(max_depth)\n",
    "\n",
    "target = torch.clamp(target, min=min_depth, max=max_depth)\n",
    "pred = torch.clamp(pred, min=min_depth, max=max_depth)\n",
    "\n",
    "thresh = torch.max((target / pred), (pred / target))\n",
    "\n",
    "d1 = torch.sum(thresh < 1.25).float() / len(thresh)\n",
    "d2 = torch.sum(thresh < 1.25 ** 2).float() / len(thresh)\n",
    "d3 = torch.sum(thresh < 1.25 ** 3).float() / len(thresh)\n",
    "\n",
    "diff = pred - target\n",
    "diff_log = torch.log(pred) - torch.log(target)\n",
    "\n",
    "abs_rel = torch.mean(torch.abs(diff) / target)\n",
    "sq_rel = torch.mean(torch.pow(diff, 2) / target)\n",
    "\n",
    "rmse = torch.sqrt(torch.mean(torch.pow(diff, 2)))\n",
    "rmse_log = torch.sqrt(torch.mean(torch.pow(diff_log , 2)))\n",
    "\n",
    "log10 = torch.mean(torch.abs(torch.log10(pred) - torch.log10(target)))\n",
    "silog = torch.sqrt(torch.pow(diff_log, 2).mean() - 0.5 * torch.pow(diff_log.mean(), 2))\n",
    "\n",
    "ms = {'d1': d1.item(), 'd2': d2.item(), 'd3': d3.item(), 'abs_rel': abs_rel.item(), 'sq_rel': sq_rel.item(), \n",
    "        'rmse': rmse.item(), 'rmse_log': rmse_log.item(), 'log10':log10.item(), 'silog':silog.item()}\n",
    "print(ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.metric import eval_depth\n",
    "target = torch.from_numpy(np.load(t_file))\n",
    "pred = torch.from_numpy(np.load(p_file))\n",
    "assert pred.shape == target.shape\n",
    "print(pred.min(), pred.max())\n",
    "\n",
    "ms = eval_depth(pred, target, dataset=\"mvsec\")\n",
    "print(ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_depth(depth, reg_factor, d_min, d_max):\n",
    "    # Normalize depth\n",
    "    depth = np.clip(depth, d_min, d_max)\n",
    "    depth = depth / d_max\n",
    "    depth = np.log(depth) / reg_factor + 1.0\n",
    "    depth = depth.clip(0.0, 1.0)\n",
    "    return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from os.path import join\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from util.metric import eval_depth, convert_nl2abs_depth\n",
    "\n",
    "reg_factor, d_min, d_max = 3.70378, 1.97041, 80\n",
    "results = {\n",
    "    \"d1\": torch.tensor([0.0]).cuda(),\n",
    "    \"d2\": torch.tensor([0.0]).cuda(),\n",
    "    \"d3\": torch.tensor([0.0]).cuda(),\n",
    "    \"abs_rel\": torch.tensor([0.0]).cuda(),\n",
    "    \"sq_rel\": torch.tensor([0.0]).cuda(),\n",
    "    \"rmse\": torch.tensor([0.0]).cuda(),\n",
    "    \"rmse_log\": torch.tensor([0.0]).cuda(),\n",
    "    \"log10\": torch.tensor([0.0]).cuda(),\n",
    "    \"silog\": torch.tensor([0.0]).cuda(),\n",
    "}\n",
    "predictions_dataset = \"/home/sph/event/da2-prompt-tuning/results/test/epde_nl_mvsec_2_decoder_mvsec_night1_3/npy\"\n",
    "target_dataset = \"/data_nvme/sph/mvsec_processed/outdoor_night1/depths\"\n",
    "prediction_files = sorted(glob.glob(join(predictions_dataset, \"*.npy\")))\n",
    "target_files = sorted(glob.glob(join(target_dataset, \"*.npy\")))\n",
    "\n",
    "num_it = len(target_files)\n",
    "nsamples = 0\n",
    "for idx in tqdm.tqdm(range(num_it)):\n",
    "    p_file, t_file = prediction_files[idx], target_files[idx]\n",
    "\n",
    "    # target = torch.from_numpy(np.load(t_file))\n",
    "    target = np.load(t_file)\n",
    "    target = prepare_depth(target, reg_factor=reg_factor, d_min=d_min, d_max=d_max)\n",
    "    target = convert_nl2abs_depth(target, clip_distance=d_max, reg_factor=reg_factor)\n",
    "    target = torch.from_numpy(target)\n",
    "    pred = torch.from_numpy(np.load(p_file))\n",
    "\n",
    "    cur_results = eval_depth(pred, target, dataset=\"mvsec\")\n",
    "    for k in results.keys():\n",
    "        results[k] += cur_results[k]\n",
    "    nsamples += 1\n",
    "\n",
    "cur_results = {}\n",
    "for k in results.keys():\n",
    "    cur_results[k] = (results[k] / nsamples).item()\n",
    "print(cur_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.epde_modal_metric import EPDE\n",
    "import torch\n",
    "from model.epde.utils import clean_pretrained_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"/home/sph/event/da2-prompt-tuning/exp/epde_nl_mvsec_2_decoder_20250116_222954/abs_rel-0.26239070296287537-6.pth\"\n",
    "\n",
    "model = EPDE(\n",
    "    model_name=\"vitl\",\n",
    "    max_depth=1,\n",
    "    event_voxel_chans=3,\n",
    "    return_feature=False\n",
    ")\n",
    "\n",
    "latest_ckp = torch.load(p, map_location=\"cpu\")\n",
    "latest_ckp = latest_ckp[\"model\"]\n",
    "model.load_state_dict(latest_ckp, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.module.state_dict()\n",
    "print(type(model))\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.utils import compare_model_weights\n",
    "p1 = \"/home/sph/event/da2-prompt-tuning/exp/epde_nl_mvsec_2_decoder_20250117_165144/abs_rel-0.5710307359695435-0.pth\"\n",
    "p2 = \"/home/sph/event/da2-prompt-tuning/exp/epde_nl_mvsec_2_decoder_20250117_165730/abs_rel-0.5476359128952026-0.pth\"\n",
    "\n",
    "c1 = torch.load(p1, map_location=\"cpu\")\n",
    "c1 = c1[\"model\"]\n",
    "\n",
    "# compare_model_weights(c1, c2, \"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from util.utils import compare_model_weights\n",
    "from model.epde_modal_metric import EPDE\n",
    "import copy\n",
    "\n",
    "p1 = \"/home/sph/event/da2-prompt-tuning/exp/epde_nl_mvsec_2_decoder_20250117_165144/abs_rel-0.5710307359695435-0.pth\"\n",
    "\n",
    "c1 = torch.load(p1, map_location=\"cpu\")\n",
    "c1 = c1[\"model\"]\n",
    "\n",
    "model = EPDE(\n",
    "    model_name=\"vitl\",\n",
    "    max_depth=1,\n",
    "    event_voxel_chans=3,\n",
    "    return_feature=False\n",
    ")\n",
    "\n",
    "model.load_state_dict(c1, strict=True)\n",
    "c2 = model.state_dict()\n",
    "\n",
    "c11 = copy.deepcopy(c1)\n",
    "c22 = copy.deepcopy(c2)\n",
    "# model.eval()\n",
    "compare_model_weights(c11, c22, \"test2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_weights(state_dict1, state_dict2):\n",
    "    # Compare the differences between two state_dict objects \n",
    "    # (whether they have the same keys and the same values).\n",
    "    keys1 = set(state_dict1.keys())\n",
    "    keys2 = set(state_dict2.keys())\n",
    "\n",
    "    missing_in_model1 = keys2 - keys1  # Keys present in model2 but not in model1\n",
    "    missing_in_model2 = keys1 - keys2  # Keys present in model1 but not in model2\n",
    "\n",
    "    all_match = True\n",
    "\n",
    "    if missing_in_model1 or missing_in_model2:\n",
    "        all_match = False\n",
    "        print(\"State dict keys do not match.\\n\")\n",
    "\n",
    "        if missing_in_model1:\n",
    "            print(f\"Keys missing in model1: {missing_in_model1}\\n\")\n",
    "\n",
    "        if missing_in_model2:\n",
    "            print(f\"Keys missing in model2: {missing_in_model2}\\n\")\n",
    "        \n",
    "    common_keys = keys1.intersection(keys2)\n",
    "    for key in common_keys:\n",
    "        if not torch.allclose(state_dict1[key], state_dict2[key]):\n",
    "            all_match = False\n",
    "            print(f\"Weight mismatch found at layer: {key}\\n\")\n",
    "            print(f\"Model 1 tensor: {state_dict1[key]}\\n\")\n",
    "            print(f\"Model 2 tensor: {state_dict2[key]}\\n\")\n",
    "            print(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "    if all_match:\n",
    "            print(\"All weights match.\")\n",
    "    return all_match\n",
    "\n",
    "\n",
    "checkpoint_path = \"...\"\n",
    "# This checkpoint contains all the weights of the model, \n",
    "# including those belonging to LoRA and those of the pre-trained model.\n",
    "ckp = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "\n",
    "# The model contains layers of lora.Linear().\n",
    "model = Model(...)\n",
    "# Loading weights in training mode may lead to anomalies.\n",
    "model.train()\n",
    "model.load_state_dict(ckp, strict=True)\n",
    "ckp2= model.state_dict()\n",
    "\n",
    "# This is very strange. If I execute model.eval(), \n",
    "# ckp and ckp2 are different; if I remove it, they are the same.\n",
    "model.eval()\n",
    "compare_model_weights(ckp, ckp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(image, mean=0, sigma=0.05):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to multi-channel data with automatic range adaptation.\n",
    "    Handles input shape [B, C, H, W] where:\n",
    "    - B: Batch size\n",
    "    - C: Channel number\n",
    "    - H: Height\n",
    "    - W: Width\n",
    "    \n",
    "    The noise level (sigma) should be specified in normalized [0,1] range.\n",
    "    \"\"\"\n",
    "    # Store original dtype and convert to float32 for calculations\n",
    "    original_dtype = image.dtype\n",
    "    image = image.astype(np.float32)\n",
    "    \n",
    "    # Calculate min/max per sample and channel while keeping dimensions\n",
    "    # data_min = np.min(image, axis=(2, 3), keepdims=True)\n",
    "    # data_max = np.max(image, axis=(2, 3), keepdims=True)\n",
    "    data_min = np.min(image)\n",
    "    data_max = np.max(image)\n",
    "    \n",
    "    # Normalize to [0, 1] range with epsilon to avoid division by zero\n",
    "    normalized = (image - data_min) / (data_max - data_min + 1e-8)\n",
    "    \n",
    "    # Generate Gaussian noise in normalized space\n",
    "    noise = np.random.normal(mean, sigma, image.shape)\n",
    "    print(f\"noise: {noise.min()}, {noise.max()}\")\n",
    "    # Add noise and clip to valid normalized range\n",
    "    noisy_normalized = np.clip(normalized + noise, 0, 1)\n",
    "    \n",
    "    # Restore original data range\n",
    "    noisy_image = noisy_normalized * (data_max - data_min) + data_min\n",
    "    \n",
    "    # Convert back to original data type\n",
    "    return noisy_image.astype(original_dtype)\n",
    "\n",
    "def change_brightness(image, factor=0.5):\n",
    "    image_float = image.astype(np.float32) * factor\n",
    "    return image_float.round().clip(0, 255).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.vis import vis_voxelgrid\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.transform import Image_Corruption\n",
    "\n",
    "imgcor = Image_Corruption(\n",
    "            mask_pro=0.5, cor_types=[\"blur\", \"mask\", \"overexpose\"]\n",
    "        )\n",
    "voxcor = Image_Corruption(\n",
    "            mask_pro=0.5, cor_types=[\"blur\", \"mask\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"/data_nvme/sph/EventScape_processed/Town01-03_train/Town01/sequence_0/events/voxels/01_000_0010_events.npy\"\n",
    "# p = \"/data_nvme/sph/mvsec_processed/outdoor_day1/voxels/00007.npy\"\n",
    "vox = np.load(p)\n",
    "print(vox.min(), vox.max(), vox.shape)\n",
    "vis_voxelgrid(vox)\n",
    "vox = vox.transpose(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vox = voxcor(image=vox)\n",
    "print(vox.shape, vox.min(), vox.max())\n",
    "vox = vox.transpose(2, 0, 1)\n",
    "vis_voxelgrid(vox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/data_nvme/sph/EventScape_processed/Town01-03_train/Town01/sequence_2/rgb/data/01_002_0005_image.png\"\n",
    "image = cv2.imread(img_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "print(image.shape, image.min(), image.max(), image.dtype)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = change_brightness(image=image, factor=1.5)\n",
    "print(image.shape, image.dtype, image.min(), image.max())\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = imgcor(image=image)\n",
    "print(image.shape, image.min(), image.max())\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (1, 1)\n",
    "import random\n",
    "b = random.uniform(a[0], a[1])\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.transform import Image_Corruption\n",
    "import cv2\n",
    "#  [\"blur\", \"overexpose\", \"mask\"])\n",
    "img_cor1 = Image_Corruption(mask_pro=1, num_masks=1, mask_radio=0.3, cor_types=[\"overexpose\"], brightness_range=[0.5, 1.5])\n",
    "img_cor2 = Image_Corruption(mask_pro=1, num_masks=1, mask_radio=0.3, cor_types=[\"blur\"], brightness_range=[0.5, 1.5])\n",
    "img_cor3 = Image_Corruption(mask_pro=1, num_masks=1, mask_radio=0.3, cor_types=[\"mask\"], brightness_range=[0.5, 1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = [\n",
    "    \"/data_nvme/sph/EventScape_processed/Town01-03_train/Town01/sequence_2/rgb/data/01_002_0004_image.png\",\n",
    "    # \"/data_nvme/sph/EventScape_processed/Town01-03_train/Town03/sequence_7/rgb/data/03_007_0003_image.png\",\n",
    "    \"/data_nvme/sph/EventScape_processed/Town01-03_train/Town01/sequence_5/rgb/data/01_005_0004_image.png\",\n",
    "    \"/data_nvme/sph/EventScape_processed/Town01-03_train/Town02/sequence_20/rgb/data/02_020_0025_image.png\",\n",
    "]\n",
    "\n",
    "for i in range(len(ps)):\n",
    "    img_path = ps[i]\n",
    "    \n",
    "    image = cv2.imread(img_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    image = img_cor1(image)\n",
    "    image = img_cor2(image)\n",
    "    image = img_cor3(image)\n",
    "    \n",
    "    output_path = f'{i}_img_cor.png'\n",
    "\n",
    "    cv2.imwrite(output_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vox_cor1 = Image_Corruption(mask_pro=1, num_masks=1, mask_radio=0.3, cor_types=[\"mask\"])\n",
    "vox_cor2 = Image_Corruption(mask_pro=1, num_masks=1, mask_radio=0.3, cor_types=[\"blur\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    img_path = f\"/home/sph/event/da2-prompt-tuning/dataset/{i}_e.png\"\n",
    "    \n",
    "    image = cv2.imread(img_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    image = vox_cor1(image)\n",
    "    image = vox_cor2(image)\n",
    "    \n",
    "    output_path = f'{i}_e_cor.png'\n",
    "\n",
    "    cv2.imwrite(output_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Bingxin Ke\n",
    "# Last modified: 2024-01-11\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def align_depth_least_square(\n",
    "    gt_arr: np.ndarray,\n",
    "    pred_arr: np.ndarray,\n",
    "    valid_mask_arr: np.ndarray,\n",
    "    return_scale_shift=True,\n",
    "    max_resolution=None,\n",
    "):\n",
    "    ori_shape = pred_arr.shape  # input shape\n",
    "\n",
    "    gt = gt_arr.squeeze()  # [H, W]\n",
    "    pred = pred_arr.squeeze()\n",
    "    valid_mask = valid_mask_arr.squeeze()\n",
    "\n",
    "    # Downsample\n",
    "    if max_resolution is not None:\n",
    "        scale_factor = np.min(max_resolution / np.array(ori_shape[-2:]))\n",
    "        if scale_factor < 1:\n",
    "            downscaler = torch.nn.Upsample(scale_factor=scale_factor, mode=\"nearest\")\n",
    "            gt = downscaler(torch.as_tensor(gt).unsqueeze(0)).numpy()\n",
    "            pred = downscaler(torch.as_tensor(pred).unsqueeze(0)).numpy()\n",
    "            valid_mask = (\n",
    "                downscaler(torch.as_tensor(valid_mask).unsqueeze(0).float())\n",
    "                .bool()\n",
    "                .numpy()\n",
    "            )\n",
    "\n",
    "    assert (\n",
    "        gt.shape == pred.shape == valid_mask.shape\n",
    "    ), f\"{gt.shape}, {pred.shape}, {valid_mask.shape}\"\n",
    "\n",
    "    gt_masked = gt[valid_mask].reshape((-1, 1))\n",
    "    pred_masked = pred[valid_mask].reshape((-1, 1))\n",
    "\n",
    "    # numpy solver\n",
    "    _ones = np.ones_like(pred_masked)\n",
    "    A = np.concatenate([pred_masked, _ones], axis=-1)\n",
    "    X = np.linalg.lstsq(A, gt_masked, rcond=None)[0]\n",
    "    scale, shift = X\n",
    "\n",
    "    aligned_pred = pred_arr * scale + shift\n",
    "\n",
    "    # restore dimensions\n",
    "    aligned_pred = aligned_pred.reshape(ori_shape)\n",
    "\n",
    "    if return_scale_shift:\n",
    "        return aligned_pred, scale, shift\n",
    "    else:\n",
    "        return aligned_pred\n",
    "\n",
    "\n",
    "# ******************** disparity space ********************\n",
    "def depth2disparity(depth, return_mask=False):\n",
    "    if isinstance(depth, torch.Tensor):\n",
    "        disparity = torch.zeros_like(depth)\n",
    "    elif isinstance(depth, np.ndarray):\n",
    "        disparity = np.zeros_like(depth)\n",
    "    non_negtive_mask = depth > 0\n",
    "    disparity[non_negtive_mask] = 1.0 / depth[non_negtive_mask]\n",
    "    if return_mask:\n",
    "        return disparity, non_negtive_mask\n",
    "    else:\n",
    "        return disparity\n",
    "\n",
    "\n",
    "def disparity2depth(disparity, **kwargs):\n",
    "    return depth2disparity(disparity, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment = \"\"\n",
    "# Align with GT using least square\n",
    "if \"least_square\" == alignment:\n",
    "    depth_pred, scale, shift = align_depth_least_square(\n",
    "        gt_arr=depth_raw,\n",
    "        pred_arr=depth_pred,\n",
    "        valid_mask_arr=valid_mask,\n",
    "        return_scale_shift=True,\n",
    "        max_resolution=alignment_max_res,\n",
    "    )\n",
    "elif \"least_square_disparity\" == alignment:\n",
    "    # convert GT depth -> GT disparity\n",
    "    gt_disparity, gt_non_neg_mask = depth2disparity(\n",
    "        depth=depth_raw, return_mask=True\n",
    "    )\n",
    "    # LS alignment in disparity space\n",
    "    pred_non_neg_mask = depth_pred > 0\n",
    "    valid_nonnegative_mask = valid_mask & gt_non_neg_mask & pred_non_neg_mask\n",
    "\n",
    "    disparity_pred, scale, shift = align_depth_least_square(\n",
    "        gt_arr=gt_disparity,\n",
    "        pred_arr=depth_pred,\n",
    "        valid_mask_arr=valid_nonnegative_mask,\n",
    "        return_scale_shift=True,\n",
    "        max_resolution=alignment_max_res,\n",
    "    )\n",
    "    # convert to depth\n",
    "    disparity_pred = np.clip(\n",
    "        disparity_pred, a_min=1e-3, a_max=None\n",
    "    )  # avoid 0 disparity\n",
    "    depth_pred = disparity2depth(disparity_pred)\n",
    "\n",
    "# Clip to dataset min max\n",
    "depth_pred = np.clip(\n",
    "    depth_pred, a_min=dataset.min_depth, a_max=dataset.max_depth\n",
    ")\n",
    "\n",
    "# clip to d > 0 for evaluation\n",
    "depth_pred = np.clip(depth_pred, a_min=1e-6, a_max=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.random.random(10)\n",
    "print(a.shape)\n",
    "a = a.reshape((-1, 1))\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def prepare_depth(depth, reg_factor, d_max, eps=1e-6):\n",
    "    # Normalize depth\n",
    "    # depth = np.clip(depth, 0.0, d_max)\n",
    "    depth = depth / d_max\n",
    "    depth = np.log(depth + eps) / reg_factor + 1.0\n",
    "    # depth = depth.clip(0.0, 1.0)\n",
    "    return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array((1.97, 1, 50, 80, 90), dtype=np.float32)\n",
    "reg_factor = 3.70378\n",
    "d_max = 80\n",
    "\n",
    "a = prepare_depth(a, reg_factor, d_max)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_Corruption(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # noise_pro=0.3,\n",
    "        mask_pro=0.3,\n",
    "        num_masks=3,\n",
    "        mask_radio=0.2,\n",
    "        exposure_alpha=2.5,\n",
    "        exposure_beta=200,\n",
    "        blur_size=25,\n",
    "        blur_sigmaX=2,\n",
    "        brightness_range=(1, 1),\n",
    "        cor_types=[\"blur\", \"overexpose\", \"mask\"],\n",
    "    ):\n",
    "        self.mask_pro = mask_pro\n",
    "\n",
    "        self.num_masks = num_masks\n",
    "        self.mask_radio = mask_radio\n",
    "\n",
    "        self.exposure_alpha = exposure_alpha\n",
    "        self.exposure_beta = exposure_beta\n",
    "\n",
    "        self.blur_size = blur_size\n",
    "        self.blur_sigmaX = blur_sigmaX\n",
    "\n",
    "        self.brightness_range = brightness_range \n",
    "        self.cor_types = cor_types\n",
    "\n",
    "    def generate_masks(self, image_shape, num_masks, width_ratio=0.2, height_ratio=0.2):\n",
    "        assert width_ratio <= 1\n",
    "        assert height_ratio <= 1\n",
    "\n",
    "        height, width, _ = image_shape\n",
    "        masks = []\n",
    "\n",
    "        # Convert ratios to actual dimensions\n",
    "        w = int(width * width_ratio)\n",
    "        h = int(height * height_ratio)\n",
    "\n",
    "        for _ in range(num_masks):\n",
    "            mask = np.zeros((height, width), dtype=np.uint8)\n",
    "            # Generate a rectangle mask with size based on the percentage of the image dimensions\n",
    "            x = random.randint(0, width - w) if width > w else 0\n",
    "            y = random.randint(0, height - h) if height > h else 0\n",
    "            cv2.rectangle(mask, (x, y), (x + w, y + h), 255, -1)\n",
    "\n",
    "            masks.append(mask)\n",
    "\n",
    "        return masks\n",
    "\n",
    "    def apply_gaussian_blur_region(self, image, mask, k_size=(15, 15), sigmaX=0):\n",
    "        if isinstance(k_size, int):\n",
    "            k_size = (k_size, k_size)\n",
    "        blurred = cv2.GaussianBlur(image, k_size, sigmaX)\n",
    "        return np.where(mask[..., None] == 255, blurred, image)\n",
    "\n",
    "    def apply_overexposure_region(self, image, mask, alpha=1.5, beta=50):\n",
    "        overexposed = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "        return np.where(mask[..., None] == 255, overexposed, image)\n",
    "\n",
    "    def mask_region(self, image, mask, mask_color=(0, 0, 0)):\n",
    "        colored_mask = np.zeros_like(image)\n",
    "        colored_mask[:] = mask_color\n",
    "        return np.where(mask[..., None] == 255, colored_mask, image)\n",
    "\n",
    "    def change_brightness(self, image, factor=0.5):\n",
    "        image_float = image.astype(np.float32) * factor\n",
    "        return image_float.round().clip(0, 255).astype(np.uint8)\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        # Change Image Brightness\n",
    "        brightness_factor = random.uniform(self.brightness_range[0], self.brightness_range[1])\n",
    "        image = self.change_brightness(image=image, factor=brightness_factor)\n",
    "        \n",
    "        if random.random() <= self.mask_pro:\n",
    "            masks = self.generate_masks(\n",
    "                image_shape=image.shape,\n",
    "                num_masks=self.num_masks,\n",
    "                width_ratio=self.mask_radio,\n",
    "                height_ratio=self.mask_radio,\n",
    "            )\n",
    "            effect_type = random.choice(self.cor_types)\n",
    "            for mask in masks:\n",
    "                if effect_type == \"blur\":\n",
    "                    image = self.apply_gaussian_blur_region(\n",
    "                        image,\n",
    "                        mask,\n",
    "                        k_size=self.blur_size,\n",
    "                        sigmaX=self.blur_sigmaX,\n",
    "                    )\n",
    "                elif effect_type == \"overexpose\":\n",
    "                    image = self.apply_overexposure_region(\n",
    "                        image, mask, alpha=self.exposure_alpha, beta=self.exposure_beta\n",
    "                    )\n",
    "                elif effect_type == \"mask\":\n",
    "                    image = self.mask_region(image, mask, mask_color=(0, 0, 0))\n",
    "                    # image = self.mask_region(image, mask, mask_color=(255, 255, 255))\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_cor_process = Image_Corruption()\n",
    "voxel_cor_process = Image_Corruption()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"/home/sph/event/da2-prompt-tuning/dataset/splits/dense/test.txt\"\n",
    "with open(p, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    p = line.strip()\n",
    "    ip, dp, vp = p.split()[0], p.split()[1], p.split()[2]\n",
    "\n",
    "    image = cv2.imread(ip)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_cor = image\n",
    "    image_cor = image_cor_process(image_cor)\n",
    "    \n",
    "    event_voxel = np.load(vp)\n",
    "    event_voxel = event_voxel.transpose(1, 2, 0)\n",
    "    event_voxel = voxel_cor_process(event_voxel)\n",
    "    event_voxel = event_voxel.transpose(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "# from dataset.transform import Image_Corruption\n",
    "import random\n",
    "def mask_left(image):\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    left_half = width // 2\n",
    "\n",
    "    image[:, :left_half] = 0\n",
    "    return image\n",
    "\n",
    "def mask_right(image):\n",
    "    height, width = image.shape[:2]\n",
    "\n",
    "    left_half = width // 2\n",
    "\n",
    "    image[:, left_half:] = 0\n",
    "    return image\n",
    "\n",
    "img_cor = Image_Corruption(mask_pro=1, mask_radio=0.3, brightness_range=(0.5, 1.5), cor_types=[\"blur\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corimg(img_path, img_corer):\n",
    "    image = cv2.imread(img_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = img_corer(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cor = Image_Corruption(mask_pro=1, num_masks=1, mask_radio=1, cor_types=[\"overexpose\"], exposure_beta=100, exposure_alpha=2)\n",
    "# img_cor = Image_Corruption(mask_pro=1, num_masks=2, mask_radio=0.3, cor_types=[\"mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = \"/data_nvme/sph/DENSE/test/seq0/rgb/frames\"\n",
    "out_dir = \"/data_nvme/sph/DENSE/test/seq0/rgb/frames_exp100\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "imgs = os.listdir(in_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in imgs:\n",
    "    if p.endswith(\"txt\"):\n",
    "        continue\n",
    "    pa = os.path.join(in_dir, p)\n",
    "    \n",
    "    image = cv2.imread(pa)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # image = mask_left(image)\n",
    "    image = img_cor(image)\n",
    "    \n",
    "    out_p = os.path.join(out_dir, p)\n",
    "    cv2.imwrite(out_p, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = [\n",
    "    \"/home/sph/event/da2-prompt-tuning/dataset/splits/mvsec/outdoor_day1.txt\",\n",
    "    \"/home/sph/event/da2-prompt-tuning/dataset/splits/mvsec/outdoor_day2.txt\",\n",
    "    \"/home/sph/event/da2-prompt-tuning/dataset/splits/mvsec/outdoor_night1.txt\",\n",
    "    \"/home/sph/event/da2-prompt-tuning/dataset/splits/mvsec/outdoor_night2.txt\",\n",
    "    \"/home/sph/event/da2-prompt-tuning/dataset/splits/mvsec/outdoor_night3.txt\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total = []\n",
    "# for sp in path_list:\n",
    "#     with open(sp, 'r') as f:\n",
    "#         lines = f.readlines()\n",
    "#         total.extend(lines)\n",
    "\n",
    "# sel = random.sample(total, 1000)\n",
    "\n",
    "with open(\"/home/sph/event/da2-prompt-tuning/da2_metric_depth/dataset/splits/mvsec/sel.txt\", 'r') as f:\n",
    "    sel = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = \"/home/sph/event/da2-prompt-tuning/dataset/splits/mvsec\"\n",
    "# sel_splits = os.path.join(dir_, \"sel.txt\")\n",
    "# with open(sel_splits, 'w') as f:\n",
    "#     f.writelines(sel)\n",
    "\n",
    "out_dir = \"/data_nvme/sph/mvsec_processed/img_blur\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "new_lines = []\n",
    "for line in sel:\n",
    "    img_p = line.split()[0]\n",
    "    dep_p = line.split()[1]\n",
    "    vox_p = line.split()[2].strip()\n",
    "\n",
    "    cored_img = corimg(img_p, img_cor)\n",
    "    \n",
    "    scene = img_p.split(\"/\")[-3]\n",
    "    name = img_p.split(\"/\")[-1]\n",
    "    out_path = os.path.join(out_dir, f\"{scene}_{name}\")\n",
    "    \n",
    "    cv2.imwrite(out_path, cv2.cvtColor(cored_img, cv2.COLOR_RGB2BGR))\n",
    "    new_lines.append(f\"{out_path} {dep_p} {vox_p}\\n\")\n",
    "\n",
    "cor_split = os.path.join(dir_, \"blur.txt\")\n",
    "with open(cor_split, 'w') as f:\n",
    "    f.writelines(new_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sel))\n",
    "print(cor_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "# from dataset.transform import Image_Corruption\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Image_Corruption(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_masks=3,\n",
    "        mask_radio=0.2,\n",
    "    ):\n",
    "\n",
    "        self.num_masks = num_masks\n",
    "        self.mask_radio = mask_radio\n",
    "\n",
    "    def generate_masks(self, image_shape, num_masks, width_ratio=0.2, height_ratio=0.2):\n",
    "        assert width_ratio <= 1\n",
    "        assert height_ratio <= 1\n",
    "\n",
    "        height, width, _ = image_shape\n",
    "        masks = []\n",
    "\n",
    "        # Convert ratios to actual dimensions\n",
    "        w = int(width * width_ratio)\n",
    "        h = int(height * height_ratio)\n",
    "\n",
    "        for _ in range(num_masks):\n",
    "            mask = np.zeros((height, width), dtype=np.uint8)\n",
    "            # Generate a rectangle mask with size based on the percentage of the image dimensions\n",
    "            x = random.randint(0, width - w) if width > w else 0\n",
    "            y = random.randint(0, height - h) if height > h else 0\n",
    "            cv2.rectangle(mask, (x, y), (x + w, y + h), 255, -1)\n",
    "\n",
    "            masks.append(mask)\n",
    "\n",
    "        return masks\n",
    "\n",
    "    def mask_region(self, image, mask, mask_color=(0, 0, 0)):\n",
    "        colored_mask = np.zeros_like(image)\n",
    "        colored_mask[:] = mask_color\n",
    "        return np.where(mask[..., None] == 255, colored_mask, image)\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        \n",
    "        masks = self.generate_masks(\n",
    "            image_shape=image.shape,\n",
    "            num_masks=self.num_masks,\n",
    "            width_ratio=self.mask_radio,\n",
    "            height_ratio=self.mask_radio,\n",
    "        )\n",
    "        for mask in masks:\n",
    "            image = self.mask_region(image, mask, mask_color=(0, 0, 0))\n",
    "\n",
    "        return image, masks\n",
    "    \n",
    "img_cor = Image_Corruption(mask_radio=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = \"/data_nvme/sph/DENSE/test/seq0/events/voxels\"\n",
    "eve_frame = \"/data_nvme/sph/DENSE/test/seq0/events/frames_white\"\n",
    "out_dir = \"/data_nvme/sph/DENSE/test/seq0/events/voxels_cor\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "voxs = os.listdir(in_dir)\n",
    "for p in imgs:\n",
    "    if p.endswith(\"txt\"):\n",
    "        continue\n",
    "    pa = os.path.join(in_dir, p)\n",
    "    \n",
    "    event_voxel = np.load(pa)\n",
    "    event_voxel = event_voxel.transpose(1, 2, 0)\n",
    "    \n",
    "    event_voxel, masks = img_cor(event_voxel)\n",
    "    event_voxel = event_voxel.transpose(2, 0, 1)\n",
    "    \n",
    "    out_p = os.path.join(out_dir, p)\n",
    "    np.save(out_p, event_voxel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"/data_nvme/sph/DENSE/test/seq0/events/data/events_0000000000.npy\"\n",
    "e = np.load(p)\n",
    "print(e.shape, e.dtype)\n",
    "print(e[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
